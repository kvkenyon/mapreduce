

Workers:

1. Workers should be blind to if they reduce or map. They just execute tasks. Given a function name they look
   up the function in either registry (reducer/mapper) and execute the function. If no function is found they fail.
  - This leads to an additional requirement that mapper + reducer need to all have unique names (Think the type system handles this already?)
2. We need to write mapper emitters to a canocial intermediate file name in /tmp/mapreduce/jobs/<uuid>/map/
3. We need to implement a message sending system for master to send tasks to workers, and other messages like
   map output is ready to be consumed etc.
4. We can associate the MapReduceOutput with the Task (like with input split)
   a. We need differennt task types
5. Let's make the default emitters file writers (not just stdout writers)


Wondering if we can use configuration instead of registry? Not the right solution. We want the library user to set the values and have them
propagate to all the machines...


Assigning tasks to machines (Workers):

If we have 1 machine then there's nothing to do: give the machine all the tasks
Sequentially execute all the maps and then the reduces.

If we have M  map tasks and R reduce tasks. 


For example the paper mentions running like this:

M = 200,000
R = 5000
Machines = 2000

Then a smaller example would look like this (factoring out 1000):
M' = 200
R' = 5
Machines' = 2

Optimal scheduling:

5 Reduce tasks for 200 map tasks means each reduce can handle 40 Map tasks

With two machines you get no great parallelization. You can have 1 reduce and 1 map, sits idle until 1 map completes
reduce starts working, as other tasks get updated by the master as maps complete. 

Deal cards method where each task is a card:

Naive:
Hand a worker a card, go to the next one and keep going around until you are out of cards

Pairwise assignment:
For a (M,R) task pair find two idle machines: Give one the map and the other the reduce
Always split the maps and reduces across different machiens


---------------------------

We want to use the library as such:

--
let spec = MapReduceSpecification::new(...);
let map_reduce = MapReduce::new(spec);
!impl_mapper(WordCounter, "word_counter");
!impl_reducer(Adder, "adder");

map_reduce.exec().async;
--

Spawns N worker::server on remote server
Spawns 1 master::server on remote server

Master orchestrates the execution.

--

Right now I have two binaries, that I have to manually invoke to start the server. I guess I could bundle them in a docker image
and then invoke a script

--

Spawn N workers and 1 reducer via k8s.

workers and master are RPC servers

Global file storage is handled via S3

We will still store intermediate outputs on the local worker machine to stay true
to the map reduce original implementation.

--

Essentially mapreduce should create the manifest of how many workers to create and post it to the k8s
API

--

Let's start with 1 master and 2 workers (1 reduce + 1 map)

We will have separate Dockerfiles for each. Maybe use compose. We can go to k8s later, but it's a whole THING. We just
need a simple test environment for now.

--

